# Import findspark and initialize. 
import findspark
findspark.init()


# Import packages
from pyspark.sql import SparkSession
import time
from pyspark.sql.functions import avg, col, year, to_date

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()


# 1. Read in the AWS S3 bucket into a DataFrame.
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv"

spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("home_sales_revised.csv"), header=True, inferSchema=True)

# Show DataFrame
df.show()


# 2. Create a temporary view of the DataFrame.
df.createOrReplaceTempView('home_sales')


# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?
# Extract the year from the date column
df_year = df.withColumn('year', year(to_date(col('date'), 'yyyy-MM-dd')))

# Calculate avg price, and group by bedrooms and year
averages = df_year.groupBy('year','bedrooms').agg(avg('price').alias('average_price'))

# Filter for 4-bedroom houses only
four_bdrm_averages = averages.filter(averages.bedrooms == 4)

# Select year and avg price columns, sort by year
result = four_bdrm_averages.orderBy('year').select('year', 'average_price')

# Show results rounded to two decimals
result.withColumn('average_price', result['average_price'].cast('decimal(10,2)')).show()

print("The average price of a four bedroom house sold per year is shown above.")


# 4. What is the average price of a home for each year the home was built,
# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?

# Calculate avg price, and group by bedrooms and year
averages = df.groupBy('date_built','bedrooms','bathrooms').agg(avg('price').alias('average_price'))

# Filter for 3-bedroom, 3-bath houses only
three_bdrm_averages = averages.filter((averages.bedrooms == 3) & (averages.bathrooms == 3))

# Select year and avg price columns, sort by year
result = three_bdrm_averages.orderBy('date_built').select('date_built', 'average_price')

# Show results rounded to two decimals
result.withColumn('average_price', result['average_price'].cast('decimal(10,2)')).show()

print("The average price of a 3-bedroom, 3-bathroom house sold by year built is shown above.")


# 5. What is the average price of a home for each year the home was built,
# that have 3 bedrooms, 3 bathrooms, with two floors,
# and are greater than or equal to 2,000 square feet, rounded to two decimal places?

# First filter for 3-bedroom, 3-bath houses only
filtered_df = df.filter((df.bedrooms == 3) & (df.bathrooms == 3) & (df.floors == 2)
                                     & (df.sqft_living >= 2000))
# Calculate avg price, and group by bedrooms and year
averages = filtered_df.groupBy('date_built').agg(avg('price').alias('average_price'))

# Select year and avg price columns, sort by year
result = averages.orderBy('date_built').select('date_built', 'average_price')

# Show results rounded to two decimals
result.withColumn('average_price', result['average_price'].cast('decimal(10,2)')).show()

print("The average price of a 3-bedroom, 3-bathroom house sold by year built with two floors, greater than or equal to 2,000 square feet is shown above.")



# 6. What is the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000? Order by descending view rating. 
# Although this is a small dataset, determine the run time for this query.
start_time = time.time()

# Calculate avg price, and group by bedrooms and year
averages = df.groupBy('view').agg(avg('price').alias('average_price'))

# Filter for 3-bedroom, 3-bath houses only
filtered_averages = averages.filter(col('average_price') >= 350000)

# Select year and avg price columns, sort by year
result = filtered_averages.orderBy(col('view').desc()).select('view','average_price')

# Show results rounded to two decimals
result.withColumn('average_price', result['average_price'].cast('decimal(10,2)')).show()


print("--- %s seconds ---" % (time.time() - start_time))


# 7. Cache the the temporary table home_sales.



# 8. Check if the table is cached.
spark.catalog.isCached('home_sales')


# 9. Using the cached data, run the last query above, that calculates 
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000. 
# Determine the runtime and compare it to the uncached runtime.

start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))



# 10. Partition by the "date_built" field on the formatted parquet home sales data 



# 11. Read the formatted parquet data.



# 12. Create a temporary table for the parquet data.



# 13. Using the parquet DataFrame, run the last query above, that calculates 
# the average price of a home per "view" rating, rounded to two decimal places,
# having an average home price greater than or equal to $350,000. 
# Determine the runtime and compare it to the cached runtime.

start_time = time.time()



print("--- %s seconds ---" % (time.time() - start_time))


# 14. Uncache the home_sales temporary table.



# 15. Check if the home_sales is no longer cached





